{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We first split the target and features from the train and test dataset\n",
    "2. Transform the following features:\n",
    " camis: We drop this feature because these are unique identifiers\n",
    " dba: We would drop the 'dba' since we expect the words in name feature of the restaurants to be unrealted to the grading.\n",
    " boro: We will use OHE on the restaurant regions which is a categorical variable \n",
    " zipcode: Since there are so many restaurants with the same zipcodes, we would OHE it (with appropriate values for max_categories to select the most frequent 20)\n",
    " cuisine_description: OHE on the descriptions (there are not many words) which is a categorical variable\n",
    " inspection_date: We would assume that the date of the inspection is unrealted to how restaurants are graded, so we drop the 'inspection_date' feature.\n",
    " action: We would use OHE on categorical variable\n",
    " violation_code: We would use OHE on categorical variable\n",
    " violation_description: We would use Bag of Words for the text with CountVectorizer()\n",
    " critical_flag: We would use OHE on categorical variable\n",
    " score: Since 'score' is the ONLY numeric feature, We would not apply any transformation on it (no need to do scaling).\n",
    " inspection_type: We would drop the 'inspection_type' feature since we expect it does not relate to the grading target.\n",
    "3. Perform cross validations on dummy, logreg and svm:\n",
    "- Both logreg and svm are better than baseline model\n",
    "- Found logreg to have highest score - choosing logreg\n",
    "(insert image here with comparisons)\n",
    "4. Fit the logreg model to the training data to get max length of count vectorizer vocabulary. Max length was found to be : 335\n",
    "5. Perform hyper parameter tuning using randomizedsearchcv to find optimum parameters to train the logistic regression model.\n",
    "- Keeping n_iters at 10 because model takes too long train if n_iters is higher than that and ultimately crashes. Since it does not compromise the accuracy by any margin, we can have 10 iterations in the random search.\n",
    "- When using randomized search to arrive at the optimum hyperparameters, the solver failed to converge. This happens when there are fluctuating errors at each random search. If all errors are within a certain threshold, the solver has the ability to converge and give us the optimum hyperparameters. The error we get when this happens:\n",
    "ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
    "To solve this some of the below mentioned methods could work (REFERENCE : https://stackoverflow.com/questions/62658215/convergencewarning-lbfgs-failed-to-converge-status-1-stop-total-no-of-iter):\n",
    "    - Increase the number of iterations\n",
    "    - Try a different optimizer\n",
    "    - Scale your data\n",
    "    - Add engineered features\n",
    "    - Data pre-processing\n",
    "    - Add more data\n",
    "We employed the first 2 solutions mentioned above - we increased the max_iter to 2500 and used a different optimizer. For selecting the correct solver in our case, we went through the scikit documentation (https://scikit-learn.org/stable/modules/linear_model.html#solvers) to find that 'lbfgs' is the default solver in scikit learn. This does not work very well for large datasets. Since we have around 150,000 training points, our dataset is quite large. In this case the best solver we can use that could help in convergence is \"SAG\" (Stochastic Average Gradient descent). It is faster than other solvers for large datasets, when both the number of samples and the number of features are large. This finally helped in converging and finding the optimized hyper parameters.\n",
    "- Best parameters found to be : {'columntransformer__countvectorizer__max_features': 80,\n",
    " 'logisticregression__C': 0.6930605534498594,\n",
    " 'logisticregression__class_weight': 'balanced',\n",
    " 'logisticregression__solver': 'sag'}\n",
    "6. Performing cross validation using the optimum hyper parameters on logreg\n",
    "- Using max_features as 20 for one hot encoding to limit the feature space\n",
    "- performing logistic regression and f\n",
    "IMAGE : https://github.com/UBC-MDS/newyork_restaurant_grading/blob/model_script_nikita/results/images/comparison_summy_lr_lrbest.png, https://github.com/UBC-MDS/newyork_restaurant_grading/blob/model_script_nikita/results/images/fitted_best_lr_model.png\n",
    "7. Fitting the best model on training data\n",
    "8. Scoring the best model on the test data (unseen data) : 0.6491\n",
    "9. PR curve\n",
    "IMAGE : https://github.com/UBC-MDS/newyork_restaurant_grading/blob/model_script_nikita/results/logistic_regression_PR_curve.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the model\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import make_scorer, recall_score, precision_score\n",
    "\n",
    "train_df = pd.read_csv(\"../data/processed/train_df.csv\")\n",
    "test_df = pd.read_csv(\"../data/processed/test_df.csv\")\n",
    "\n",
    "# split features and target for train and test data\n",
    "\n",
    "X_train = train_df.drop(columns=[\"grade\"])\n",
    "y_train = train_df[\"grade\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"grade\"])\n",
    "y_test = test_df[\"grade\"]\n",
    "loaded_model = pickle.load(open(\"../results/finalized_model.sav\", 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "print(\"\\nCreating and saving PR curve plot...\")\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "y_test, loaded_model.predict_proba(X_test)[:, 1], pos_label=\"F\"\n",
    ")\n",
    "plt.plot(precision, recall, label=\"logistic regression: PR curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.plot(\n",
    "    precision_score(y_test, loaded_model.predict(X_test), pos_label=\"F\"),\n",
    "    recall_score(y_test, loaded_model.predict(X_test), pos_label=\"F\"),\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");\n",
    "plt.savefig('../results/' + 'logistic_regression_PR_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('nyc_rest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb107014b547d6b236687975823ab02ebaf2b1116e9a6d027894d43906e08144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
